import jaxopt
import jax
import jax.numpy as jnp

from typing import Any
from typing import Callable
from typing import NamedTuple
from typing import Optional
from typing import Union
import os
import sys

# make higher-level lib package visible to scripts
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname("src"), '..')))
from jaxopt._src.proximal_gradient import ProximalGradient, ProxGradState

A = jnp.array([[1, 2], [3, 4]])


def f(x):
    return x.T @ (A @ A.T) @ x



class BacktrackingProximalGradient(jaxopt.ProximalGradient):
   linesearch: jaxopt.BacktrackingLineSearch = None

   def _iter(self, iter_num, x, x_fun_val, x_fun_grad, stepsize, hyperparams_prox, args, kwargs):

    if self.linesearch is not None:
      next_stepsize = self.linesearch.run(init_stepsize=1.0, params=x)
      next_x = self._prox_grad(x, x_fun_grad, next_stepsize, hyperparams_prox)
      return next_x, next_stepsize
    
    return super()._iter(iter_num, x, x_fun_val, x_fun_grad, stepsize, hyperparams_prox, args, kwargs)


class BacktrackingGradientDescent(BacktrackingProximalGradient):
  """Gradient Descent solver.

  Attributes:
    fun: a smooth function of the form ``fun(parameters, *args, **kwargs)``,
      where ``parameters`` are the model parameters w.r.t. which we minimize
      the function and the rest are fixed auxiliary parameters.
    value_and_grad: whether ``fun`` just returns the value (False) or both
      the value and gradient (True).
    has_aux: whether ``fun`` outputs auxiliary data or not.
      If ``has_aux`` is False, ``fun`` is expected to be
        scalar-valued.
      If ``has_aux`` is True, then we have one of the following
        two cases.
      If ``value_and_grad`` is False, the output should be
      ``value, aux = fun(...)``.
      If ``value_and_grad == True``, the output should be
      ``(value, aux), grad = fun(...)``.
      At each iteration of the algorithm, the auxiliary outputs are stored
        in ``state.aux``.

    stepsize: a stepsize to use (if <= 0, use backtracking line search), or a
      callable specifying the **positive** stepsize to use at each iteration.
    maxiter: maximum number of proximal gradient descent iterations.
    maxls: maximum number of iterations to use in the line search.
    tol: tolerance to use.

    acceleration: whether to use acceleration (also known as FISTA) or not.
    verbose: whether to print error on every iteration or not.
      Warning: verbose=True will automatically disable jit.

    implicit_diff: whether to enable implicit diff or autodiff of unrolled
      iterations.
    implicit_diff_solve: the linear system solver to use.
  """

  linesearch: jaxopt.BacktrackingLineSearch = None
  def init_state(self,
                 init_params: Any,
                 *args,
                 **kwargs) -> jaxopt.ProxGradState:
    """Initialize the solver state.

    Args:
      init_params: pytree containing the initial parameters.
      *args: additional positional arguments to be passed to ``fun``.
      **kwargs: additional keyword arguments to be passed to ``fun``.
    Returns:
      state
    """
    return super().init_state(init_params, None, *args, **kwargs)

  def update(self,
             params: Any,
             state: NamedTuple,
             *args,
             **kwargs) -> jaxopt.base.OptStep:
    """Performs one iteration of gradient descent.

    Args:
      params: pytree containing the parameters.
      state: named tuple containing the solver state.
      *args: additional positional arguments to be passed to ``fun``.
      **kwargs: additional keyword arguments to be passed to ``fun``.
    Returns:
      (params, state)
    """
    if self.linesearch is not None:
      super().linesearch = self.linesearch
    return super().update(params, state, None, *args, **kwargs)

  def optimality_fun(self, params, *args, **kwargs):
    """Optimality function mapping compatible with ``@custom_root``."""
    return self._grad_fun(params, *args, **kwargs)

  def __post_init__(self):
    super().__post_init__()
    self.reference_signature = self.fun

    
   


def run_all(solver, w_init, *args, **kwargs):
  state = solver.init_state(w_init, *args, **kwargs)
  sol = w_init
  sols, errors = [], []
  ls = jaxopt.BacktrackingLineSearch(fun=f, maxiter=20, condition="strong-wolfe",
                            decrease_factor=0.8)

  def update(sol, state):
    ls_step, _ = ls.run(init_stepsize=1.0, params=sol)
    state._replace(stepsize=ls_step)
    return solver.update(sol, state, *args, **kwargs)

  for _ in range(solver.maxiter):
    sol, state = update(sol, state)
    sols.append(sol)
    errors.append(state.error)

  return jnp.stack(sols, axis=0), errors


def _main():
    x_init = jnp.array([1., 1.])
    gd = jaxopt.GradientDescent(fun=f, maxiter=5, acceleration=False)
    print(run_all(gd, x_init)[0])


if __name__ == '__main__':
    _main()